
\section{Research Direction: Testing}
\label{sec:testing-debugging}

Within the increasing demand for software development as a service,
\textit{software testing as a service} has seen significant growth and adoption
in its own right, often as a separate service that is independent of development
activities. In fact, according to a 2006 survey,\footnote{\scriptsize
  \url{http://www.drdobbs.com/architecture-and-design/cheapers-not-always-better/184415486?requestid=247829}}
software testing was the second largest outsourced software-engineering activity
after coding: 81\% of the 200 industrial practitioners, who participated in the
survey, stated that they outsource software testing. Given this trend, services
companies now routinely offer services that focus exclusively on testing
activities.

%% The engagement modes can vary: staff augmentation, core/flex, managed service
%% (fixed capacity, outcome based). But perhaps this needs to be mentioned earlier
%% as these modes are not particular to testing services.

\begin{figure}[t]
\centering
\includegraphics[width=\columnwidth, clip, trim = 20mm 133mm 55mm
  18mm]{figs/testing-activities.pdf}
\vspace*{-15pt}
\caption{Typical testing activities performed in delivery of testing services.}
\vspace*{-10pt}
\label{fig:testing-activities}
\end{figure}

The nature of the activities performed in a testing-services engagement can vary
from client to client; but, typically, the scope of work includes activities,
such as test design, test-data creation, test automation, test execution, and
test maintenance. Figure~\ref{fig:testing-activities} presents an overview of
the commonly performed activities in the delivery of testing services. A
delivery engagement might involve any subset of these activities. For example,
delivery for a client might involve only manual test execution, or the scope of
work may include test automation and test maintenance as well. In other cases,
the scope of work may be even broader, encompassing test design and/or test
planning. The nature of the test cases can also vary from functional tests to
tests that focus on validating non-functional system properties, such as
performance and security.

The activities shown in Figure~\ref{fig:testing-activities}, of course, pertain
to testing in general (whether performed in-house or in an outsourced manner),
but there are factors that can add unique challenges in the setting of testing
services. For instance, many of these activities can require specific skills in
testing techniques/tools or coding expertise, which the average testing
practitioner involved in service delivery may not possess. Second, some of the
activities may need to be carried out in strict time-bound cycles and
client-controlled test environments. Finally, the myriad of technologies that
must be accommodated in the contexts of the IT systems of different clients adds
another layer of complexity for service companies.

\subsection{Test Planning}
\label{sec:test-planning}

Test planning involves tasks such as establishing the test process or strategy,
allocating resources, estimating cost and schedule, defining test-environment
requirements, identifying test targets, projecting defects and test numbers for
each target, etc.  Test planning can be done at multiple levels---such as macro
planning at the start of a project which evolves into micro planning as the
project proceeds---and the test plan needs to be continuously refined during the
course of a project as more accurate information becomes available.

Test planning is intended not only to guide all downstream testing activities,
including test design, test execution, test-data and test-environment
provisioning, test reporting, etc., it should also answer key business
questions, such as~\cite{Kagan:NextGenTesting}

\begin{itemize}
\denseitems

\item How do we determine the quality of our testing effort?

\item Are we getting our money's worth out of testing?

\item Are we doing too much or too little testing?

\item Will an increased testing investment drive further improvement in quality?

\item Our testing budget was cut---what testing should we eliminate what will be
  its impact on product quality?

\end{itemize}

One of the key challenges in test planning is standardizing and automating it,
using data intelligence and forecasting. In the context of delivery, there is
the opportunity of mining and leveraging data from hundreds of client projects
to build benchmark data that can improve the accuracy of even the initial
estimates in a project.

\subsection{Test Optimization}
\label{sec:test-design}

Optimization of test cases using techniques such as combinatorial test design.

Need for test clustering techniques for manual tests.

\subsection{Test Data Generation}
\label{sec:test-data}

\input{testdata}

\subsection{Test Automation}
\label{sec:test-automation}

The functional test cases created during test design are often stated informally
in natural language. These tests are known as \textit{manual test cases}, which
consist of a sequence of steps, intended for execution by a human on the
application user interface.  Test automation is the activity of converting these
test cases into test scripts or programs that perform the test steps
mechanically. After creation, the automated test scripts can be executed in
regression cycles without any human intervention.

Although test automation is desirable (\eg for efficient and predictable test
executions) and sometimes necessary (\eg to meet time-constrained regression
schedules), creating \textit{robust} test scripts that can be used repeatedly in
regression cycles is expensive. In essence, creating such scripts is
time-consuming, involving a significant amount of coding, and requires expertise
in test automation tools (\eg \cite{hpqtp,ibmrft,selenium}). In the context of
service delivery, this poses the unique challenge: how can change-resilient test
scripts be created efficiently by delivery personnel who may not possess good
programming skills or deep tool knowledge?

%% Moreover, test automation involves not only the one-time cost of creating the
%% scripts, but also a recurring cost of maintaining the scripts in response to
%% application changes and changes in the execution environment.

While script robustness is certainly relevant in the context of evolution of the
application under test or its persistent data, it is also pertinent when a
script needs to be executed on different platforms, execution environments, or
application variants. For example, functional testing of web applications needs
to be performed on different browsers to detect potential cross-browser
incompatibility
defects~\cite{Choudhary2010,Shauvik:2012,Choudhary:2013,Mesbah:2011}. Similarly,
in the context of mobile applications, tests for a native mobile application
would need to run the variants of the application for different platforms (\eg
Android, iOS, BlackBerry). Ideally, in such situations, a robust test script
should be agnostic to browser/platform-specific differences unless, of course,
those differences are symptoms of application defects.

\subsubsection*{Research Problem: Tackling Script Fragility}

The core problem that needs to be addressed in attempts to solve the
script-fragility problem is how to locate user-interface (UI) elements in a
reliable manner. In general, there are two broad categories of techniques for
locating UI elements. The first category relies on the structure of the internal
representation of the UI~\footnote{\small In the context of web applications,
  the internal UI representation is the Document Object Model (DOM).} and on
internal attributes, such as IDs or names, of UI elements/widgets. Sample test
automation tools in this category include IBM Rational Functional
Tester~\cite{ibmrft}, HP QuickTest Professional (QTP)~\cite{hpqtp}, and
Selenium~\cite{selenium}. Over-reliance on internal structures and attributes
can make scripts fragile (\eg if the structure changes often or the attributes
are generated dynamically) and, in particular, unreliable for execution across
browsers and platforms.

The second category of tools rely on image processing to locate UI elements;
sample tools in this space include Eggplant~\cite{Eggplant} and
Sikuli~\cite{Chang:2010, Yeh:2009}. These tools record a test script as a
sequence of actions performed on images. During automation, the tester grabs a
portion of the screen around the element of interest, specifies the ``hotspot''
in this portion (the location where, \eg a click action would be performed), and
specifies the image-similarity threshold to be used during test execution to
locate the element. By being independent of internal structure and attributes,
such tools are resilient to changes in the internal structure/attributes, but
they are fragile in the presence of differences in visual rendering, say across
browsers or application variants.

A third, and less-common, alternative to these approaches is to associate with a
UI element a label in the vicinity of that element, and refer to the element via
its associated label. This approach, which is used in the \textsc{ata} tool
developed at IBM Research~\cite{thummalapenta:2012b, thummalapenta:2012a,
  thummalapenta:2013a} and also supported by QTP~\cite{hpqtp}, can overcome the
limitations of approaches that rely on internal structure or image
processing. But, the currently available implementations of these approaches
perform simple label associations (based on visual proximity only) and are
inapplicable when ambiguous labels exist. Development of more powerful and
generally applicable techniques for label association would be a fruitful
direction of research.

These approaches have different strengths and weaknesses; it is quite possible
that no one approach turns out to be the best in all circumstances. For example,
an image-based technique may outperform an internal-structure-based technique
for cross-browser test execution; but, the converse would likely be true for
test execution across internationalized variants of a web application. In
practice, the choice of the technique would have to be tailored to the
requirements of testing.  Development of tools that combine the approaches, and
rigorous empirical studies that evaluate the resiliency of these approaches for
different types of testing would make valuable research contributions.

%% Cross-browser testing: keeping up with browser upgrades, reduce reliance on DOM
%% structure and attributes.

\subsubsection*{Research Problem: Automated Test Adaptation}

Test automation in the space of mobile applications faces the form\-idable
challenge of high diversity, which occurs along multiple dimensions: many
platforms, devices, web browsers (in the case of mobile web applications), and
application variants. For instance a mobile application can have native, hybrid,
and web variants; moreover, in the case of native and hybrid applications,
different platform-specific and device-specific variants can exist.

Efficiently testing an application across different combinations requires
automation, but the automated test scripts must be self-adaptive. Native or
hybrid mobile applications typically have variants that execute on different
platforms (\eg Android, iOS, and BlackBerry) and different types of devices
(phones or tablets). For some application variants, the UI layout and flow may
be exactly the same, in which case a test script that has been recorded in a
platform-agnostic script notation (\eg \cite{PerfectoScriptOnce}) on one
application variant can be executed, without modification, on another
variant. But, in more complicated cases, the UI layout and flow for the same
scenario may differ across variants---as an archetypal example, the phone and
tablet variants of a mobile application would typically support many common
scenarios that are realized via different sequences of UI events. In such
instances, can a script automated on the tablet variant be automatically adapted
for execution on the phone variant?

The problem of test adaptation also shows up in the context of delivery
involving packaged applications, such as SAP. Typically, the implementation of a
packaged application for a client requires the customization of default data
definitions, processes, user interfaces, etc. to the client's requirements. Such
customizations modify the default flows available through the application UI,
raising the question of whether test scripts can be automatically adapted to
accommodate the changes.

We believe there is scope for the development of new techniques that perform
such test adaptation, across platforms, variants, and cutomizations, while
ensuring that the intent of the test cases are preserved.

\subsubsection*{Research Problem: Automated Test-Script Synthesis}

The problem of converting manual test cases to automated test scripts can be
looked upon as an instance of program synthesis~\cite{Gulwani:2010}.  Automated
program synthesis addresses the problem of discovering a program that realizes a
given user intent. There are three dimensions of the synthesis problem: the form
of user intent, the search space of programs, and the search
technique~\cite{Gulwani:2010}. For instance, the user intent could be stated in
different forms such as, natural language, input-output examples, logical
relations between inputs and outputs, and demonstrations. In test automation,
user intent is specified in the form of manual test steps, and the end goal is
an automated script that realizes that intent.

Recent work~\cite{thummalapenta:2012a} has attempted to automate the creation of
test scripts from manual test steps by combining natural-language processing
with dynamic exploration of multiple flows via backtracking to search for the
correct test script from the space of possible scripts. However, there are
practical limitations of that technique: dynamic exploration of alternative
flows in the application UI can encounter problems such as persistent state
updates during an exploration that may have to be undone or disabled UI elements
that limit the scope of exploration. Addressing these problems is important for
effective dynamic exploration of the space of possible test scripts.

Type-based program synthesis~\cite{Perelman:2012} and natural-language
processing have also been applied for creating automation scripts for
smartphones from natural-language descriptions of tasks~\cite{Le:2013}. Such
approaches could also be leveraged for test automation. Creation of automated
test scripts can often require the coding of custom functions; for example, a
verification step in a test might require checking that the values in a
drop-down list are sorted or that some value exists in a particular cell of a
table. Currently, such code must be written manually. Automatic synthesis of
such code, based on a specification of user intent, could go a long way in
automating the overall creation of test scripts.

\subsection{Test Maintenance}
\label{sec:test-maintenance}

Although developing scripts in a change-resilient manner, as discussed in
Section~\ref{sec:test-automation}, can reduce test-maintenance effort, it cannot
totally eliminate maintenance. As an application evolves and its test suite
grows in size, the suite can require maintenance to repair broken tests, remove
obsolete tests, and eliminate any redundancies that creep into the suite.

\subsubsection*{Research Problem: Automated Test-Flow Repair}

Refactoring of the application GUI, such as splitting a web page into multiple
tabbed pages, can break the flow of a test~\cite{thummalapenta:2013a}, which
would occur irrespective of how robust the mechanism for locating UI elements
is. Such changes require the test flow to be repaired, involving addition or
deletion of test steps---automatically performing such repairs is beyond the
capability many of existing GUI test repair techniques~\cite{Choudhary:2011,
  Grechanik:2009, Memon:2008}.

The problem of test-flow repair is similar to the problem of test adaptation
across platforms and application variants, but there are differences that can
influence the types of techniques that can be developed. In the case of
test-flow repair, two \textit{versions} of the same application are available
along with change information captured in version-management systems. Thus,
static program-differencing techniques and analysis of change logs could be
leveraged to guide flow repair. In contrast, for test adaptation across
platforms, two \textit{variants} of the application, implemented in different
programming languages (\eg Objective-C for iOS variants and Java for Android
variants), would be available; thus, static-analysis techniques may not be
readily applicable.

Proposals for GUI-refactoring-driven test repair~\cite{Daniel2011} accommodate
certain types of GUI refactorings (\eg replacing a radio box with a drop-down
list), but do not as yet address general flow repair.  Recent research has seen
the development of repair techniques for broken GUI flows~\cite{Zhang2013},
addressing some of the limitations of the existing test-repair techniques, but
there are opportunities for more research contributions on this topic---\eg
through further investigation of techniques that combine static program analysis
and dynamic GUI exploration.

\subsubsection*{Research Problem: Reducing Redundancies in Test Scripts}

Over time, as a suite of automated test scripts evolves, subtle redundancies can
creep into it in the form of similar tests. Similar tests are not redundant by
any measure; but, they contain many common actions that are executed repeatedly,
which over a large test suite, can degrade execution time substantially.  We
have seen that sometimes a service provider can inherit such test suites from
another vendor, consisting of tens of thousands of tests, with significant
opportunities for optimization of test execution.

Recent work on the idea of merging similar tests to improve test execution time,
while preserving the fault-detection capability of the original test suite, has
shown promising initial results~\cite{Devaki:2013}. But, such techniques need
further development and evaluation. For instance, test merging must take into
account application evolution because changes in the application can invalidate
some of the merging performed on the old application version. (This problem
afflicts, in general, all techniques for test-suite reduction.)  Development of
scalable and efficient techniques for automated test merging, that preserve
fault-detection capability and are evolution-aware, can be a fruitful research
direction.

