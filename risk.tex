\section{Risk Management}

Software risk management is a well established discipline~\cite{risk1,risk2} that has generated continued academic interest as the complexity and nature of the software projects have evolved over time. Traditional risk management techniques have been focused on identifying and codifying best practices that prevent or reduce the failure rate~\cite{risk3,risk4,risk5,risk6,risk7}. Large IT organizations have assimilated many of these findings in their risk management practice.  However, adopting these best practices does not guarantee that risk is eliminated or even reduced to an acceptable level -- new software development models driven by globalization, competition and an ever changing software landscape throw up new patterns of trouble. In this section we will discuss the state of the art in this area and what we see as potential open problems that need to be addressed.

Functionality risk is defined as the risk that the completed system will not meet its user's needs~\cite{risk11}.  It is the risk that churn in business requirements and scope combined with ineffective communication mechanisms leads to the development of working software that is essentially useless to the end user~\cite{risk12}. In complex IT engagements we find that reducing or eliminating functionality risk, which in turn leads to high customer satisfaction, may result in an unacceptable increase in other risk factors such as financial risk for the provider. We believe that the best approach is to look for trouble patterns across a broad spectrum of risk indicators and surface problems which then allow risk managers to make informed decisions.

It is well known that decision-makers, including those in the risk management community, routinely use flawed heuristics in decision-making, which are subject to systematic biases~\cite{risk27}. Heemstra~\cite{risk24} analyzes how filters and biases of personnel involved in judging a project may prevent an accurate risk assessment. He suggests a team based decision making approach to avoid being heavily influenced by particular individuals. Host and Lindholm~\cite{risk25}  do an empirical analysis of how a set of identified project risks are weighted by different individuals. They find a wide variance in the perceived importance attached to individual risk factors -- however, they could not explain this variance on the basis of the particular role played by the person in the project. Maytorena~\cite{risk26} provides an interesting study on the effect of experience on the ability of project managers to recognize certain risk factors. It shows that experience does not have a significant impact on the ability to detect risk as do other factors like risk management training, ability to quickly grasp information and the level of education. In practice, we find that packaging the statistical information and trouble patterns in an easily understandable and actionable fashion to the risk managers may be as important as the information itself.

Quantitative analysis of IT investment decisions using options analysis, to mitigate financial risk, is an active area of research~\cite{14,15,16}. Options analysis can be used to assess the value of prototyping work and early adoption initiatives related to new IT platforms – options provide a way to evaluate the value of IT projects which give the right to adopt the resulting technology without having the obligation to do so. Fichman~\cite{risk14} shows how options analysis can be used to predict IT platform initiation and adoption, value IT platform options and manage IT platform implementation. Chen and Sheng~\cite{risk15} discuss how to do real options analysis while accounting for estimation errors. However, the ability to use the results of the analysis in a dispassionate manner to start and cull IT projects is still a contentious topic~\cite{risk17}. As Keil et al.~\cite{risk18} observe, there is a strong bias in many organizations to continue with IT projects even though financial analysis indicates otherwise. In our view, the lack of widespread adoption for options analysis can be traced back to a fundamental issue that effects all methods for analyzing financial risk – the inability to accurately estimate with any degree of certainty the net present value (NPV) of a complex in-flight IT project. One option is to evaluate financial risk by analyzing how projects that exhibited similar trends in project metrics performed in the past -- thus, the analysis has no dependence on a particular methodology for evaluating NPV.

\subsection{Predicting Risk}

In recent literature, the trend towards a systems approach to risk management, where in statistical techniques are used to classify and mine the project metrics data, with a view to pro-actively learn the new trends is gaining acceptance~\cite{risk8,risk9,risk10}.  These methods actively use the metrics collected during traditional risk management reviews and then employ techniques borrowed from statistical learning theory to derive models that describe the relationship between the collected metrics and eventual project outcomes. Thus, certain patterns of values in project metrics can act as alerts, which can be then used to initiate further reviews to see whether the alert was justified. It is important to note that these techniques in most cases cannot identify why a project may be troubled i.e., they can only serve as a starting point for further investigation.  Another significant short coming of these models is their inability to implicitly account for outliers and missing data -- for example, a project in good health will not typically have as thorough a review as a project in trouble. Unless the data that is used to train the model is carefully validated to account for these characteristic patterns, there is a good chance that the model will not be very accurate. This problem is particularly prevalent in large IT delivery organizations which typically exhibit a large variance in both the quality of the metrics collected and reporting patterns.

 In any complex software development project, a number of risk assessment related activities are conducted prior to project inception with an objective of determining the risk entailed in delivery. The number and depth of these reviews is determined by a variety of factors, such as: the size of the project, the novelty of the proposed solution, and the industry sector to which the client belongs. These reviews may cross multiple delivery organizations within and beyond the purview of the primary service provider who is responsible for the delivery of the integrated software solution. Even for a large services provider that performs thousands of software projects with a variety of clients, it is not clear that there is infact a common set of criteria to judge the risk involved in these projects due to the great diversity in the projects and the client context in which they are executed. In this section, we will describe an approach to this problem of risk identification which has proven to be highly reliable and has been thoroughly tested in the field over the last five years.

 By reviewing the risk management reviews of several recent IT delivery projects, we identified a set of sixteen questions that were considered to be predictive of the project outcome in the initial phases of the project life cycle. These were then whetted by experienced risk managers and carefully screened to ensure that they can be used to obtain clearly defined answers within well-defined ranges. The intent was to remove subjectivity in the answers by basing them on data that is readily accessible to the risk manager.  Questions ranged from a client's past experience with the delivery organization, match of the delivery team’s technical skills to the project objectives and the type of contract (fixed price vs. time and materials).  A key point to be noted is that these questions do not delve deeply into the technical details of the project itself, as they need to cover a variety of IT projects ranging from implementing a custom application to customizing a packaged application. However, all the questions have proven to be highly correlated with the eventual outcome of the project.

 The traditional risk management approach was to scale the answers for these set of questions for a given project, sum the scaled answers and apply threshold(s)/range(s) on the result which can be then associated with a given outcome of a projected future project management review based on correlation with historical data. However, the pitfalls in such an approach are clear – dependencies/interactions between sets of questions may lead to inconsistent results over a broad set of projects. For example, a “fixed price” deal combined with a novel (first of a kind) technical solution may be more risky to undertake than a “time and materials” deal in the same context. In practice, the choice of any given set of thresholds would lead to a misclassification of a significant number of projects.

 We took an alternative approach where we used statistical classification algorithms to match the risk management questions with the eventual project outcome~\cite{risk28}. Prior to classification, the answers to the risk management review questions were scaled and binned appropriately to reduce the effects of variance due to human error and differences in procedures followed in different countries. For example, a question about the number of customer contacts prior to the signing of the deal was reduced to three classes – high, medium and low rather than asking for an exact number (since the definition of a customer contact can be interpreted and even tallied differently by different risk managers). We chose to apply a modified boosted decision tree classifier to the data set, as this approach not only produced acceptable results, but the resultant rules make the prediction process transparent to the end users. We split the available data set into a training and test data set through random sampling. The results showed that the analysis is both accurate (< 12\% false positives and < 18\% false negatives) and performs well in practice.

 We tried this and other analytical techniques to predict and manage risks in complex IT engagements within IBM over the last several years. IBM has publicly acknowledged in its 2012 annual investor's briefing that they derived an annual savings of 50 million dollars over the last three years by employing predictive analytics in the area of delivery excellence~\cite{ibm-investors-briefing}. Although we were able to predict risk, being able to manage it requires a deeper understanding of the root causes and the sensitivity of the measurable outcomes to changes in these causal factors. This is an active area of research.

\label{sec:risk}
